{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exposed-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "     \n",
    "\n",
    "class TextToTensor():\n",
    "\n",
    "    def __init__(self, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def string_to_tensor(self, string_list: list) -> list:\n",
    "        \"\"\"\n",
    "        A method to convert a string list to a tensor for a deep learning model\n",
    "        \"\"\"    \n",
    "        string_list = self.tokenizer.texts_to_sequences(string_list)\n",
    "        string_list = pad_sequences(string_list, maxlen=self.max_len)\n",
    "        \n",
    "        return string_list\n",
    "\n",
    "def clean_text(\n",
    "    string: str, \n",
    "    punctuations=r'''!()-[]{};:'\"\\,<>./?@#$%^&*_~''',\n",
    "    stop_words=[]) -> str:\n",
    "    \"\"\"\n",
    "    A method to clean text \n",
    "    \"\"\"\n",
    "    # Cleaning the urls\n",
    "    string = re.sub(r'https?://\\S+|www\\.\\S+', '', str(string))\n",
    "\n",
    "    # Cleaning the html elements\n",
    "    string = re.sub(r'<.*?>', '', string)\n",
    "\n",
    "    # Removing the punctuations\n",
    "    for x in string.lower(): \n",
    "        if x in punctuations: \n",
    "            string = string.replace(x, \"\") \n",
    "\n",
    "    # Converting the text to lower\n",
    "    string = string.lower()\n",
    "\n",
    "    # Removing stop words\n",
    "    string = ' '.join([word for word in string.split() if word not in stop_words])\n",
    "\n",
    "    # Cleaning the whitespaces\n",
    "    string = re.sub(r'\\s+', ' ', string).strip()\n",
    "\n",
    "    return string        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "other-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Input, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "\n",
    "class RnnModel():\n",
    "    \"\"\"\n",
    "    A recurrent neural network for semantic analysis\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_matrix, embedding_dim, max_len, X_additional=None):\n",
    "        \n",
    "        inp1 = Input(shape=(max_len,))\n",
    "        x = Embedding(embedding_matrix.shape[0], embedding_dim, weights=[embedding_matrix])(inp1)\n",
    "        x = LSTM(256, return_sequences=True)(x)\n",
    "        x = LSTM(128)(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "        x = Dense(64, activation=\"relu\")(x)\n",
    "        x = Dense(1, activation=\"sigmoid\")(x)    \n",
    "        model = Model(inputs=inp1, outputs=x)\n",
    "\n",
    "        model.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n",
    "        self.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "annual-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Embeddings():\n",
    "    \"\"\"\n",
    "    A class to read the word embedding file and to create the word embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, vector_dimension):\n",
    "        self.path = path \n",
    "        self.vector_dimension = vector_dimension\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_coefs(word, *arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    def get_embedding_index(self):\n",
    "        embeddings_index = dict(self.get_coefs(*o.split(\" \")) for o in open(self.path, errors='ignore'))\n",
    "        return embeddings_index\n",
    "\n",
    "    def create_embedding_matrix(self, tokenizer=None, max_features=None):\n",
    "        \"\"\"\n",
    "        A method to create the embedding matrix\n",
    "        \"\"\"\n",
    "        model_embed = self.get_embedding_index()\n",
    "\n",
    "        if max_features is None:\n",
    "            max_features = len(model_embed)\n",
    "\n",
    "        word_index = model_embed\n",
    "        if tokenizer is not None: \n",
    "            word_index = tokenizer.word_index\n",
    "\n",
    "        embedding_matrix = np.zeros((max_features + 1, self.vector_dimension))\n",
    "        for index, word in enumerate(word_index.keys()):\n",
    "            if index > max_features:\n",
    "                break\n",
    "            else:\n",
    "                try:\n",
    "                    embedding_matrix[index] = model_embed[word]\n",
    "                except:\n",
    "                    continue\n",
    "        return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "functional-huntington",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# The main model class\n",
    "#from RNN_model import RnnModel\n",
    "\n",
    "# Importing the word preprocesing class\n",
    "#from text_preprocessing import TextToTensor, clean_text\n",
    "\n",
    "# Importing the word embedding class\n",
    "#from embeddings import Embeddings\n",
    "\n",
    "# Loading the word tokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# For accuracy calculations\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "class Pipeline:\n",
    "    \"\"\"\n",
    "    A class for the machine learning pipeline\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        X_train: list, \n",
    "        Y_train: list, \n",
    "        embed_path: str, \n",
    "        embed_dim: int,\n",
    "        stop_words=[],\n",
    "        X_test=[], \n",
    "        Y_test=[],\n",
    "        max_len=None,\n",
    "        epochs=3,\n",
    "        batch_size=256\n",
    "        ):\n",
    "\n",
    "        # Preprocecing the text\n",
    "        X_train = [clean_text(text, stop_words=stop_words) for text in X_train]\n",
    "        Y_train = np.asarray(Y_train)\n",
    "        \n",
    "        # Tokenizing the text\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "        # Saving the tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Creating the embedding matrix\n",
    "        embedding = Embeddings(embed_path, embed_dim)\n",
    "        embedding_matrix = embedding.create_embedding_matrix(tokenizer, len(tokenizer.word_counts))\n",
    "\n",
    "        # Creating the padded input for the deep learning model\n",
    "        if max_len is None:\n",
    "            max_len = np.max([len(text.split()) for text in X_train])\n",
    "        TextToTensor_instance = TextToTensor(\n",
    "            tokenizer=tokenizer, \n",
    "            max_len=max_len\n",
    "            )\n",
    "        X_train = TextToTensor_instance.string_to_tensor(X_train)\n",
    "\n",
    "        # Creating the model\n",
    "        rnn = RnnModel(\n",
    "            embedding_matrix=embedding_matrix, \n",
    "            embedding_dim=embed_dim, \n",
    "            max_len=max_len\n",
    "        )\n",
    "        rnn.model.fit(\n",
    "            X_train,\n",
    "            Y_train, \n",
    "            batch_size=batch_size, \n",
    "            epochs=epochs\n",
    "        )\n",
    "\n",
    "        self.model = rnn.model\n",
    "\n",
    "        # If X_test is provided we make predictions with the created model\n",
    "        if len(X_test)>0:\n",
    "            X_test = [clean_text(text) for text in X_test]\n",
    "            X_test = TextToTensor_instance.string_to_tensor(X_test)\n",
    "            yhat = [x[0] for x in rnn.model.predict(X_test).tolist()]\n",
    "            \n",
    "            self.yhat = yhat\n",
    "\n",
    "            # If true labels are provided we calculate the accuracy of the model\n",
    "            if len(Y_test)>0:\n",
    "                self.acc = accuracy_score(Y_test, [1 if x > 0.5 else 0 for x in yhat])\n",
    "                self.f1 = f1_score(Y_test, [1 if x > 0.5 else 0 for x in yhat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-percentage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "577/577 [==============================] - 656s 1s/step - loss: 0.2656\n",
      "Epoch 2/7\n",
      "577/577 [==============================] - 643s 1s/step - loss: 0.1072\n",
      "Epoch 3/7\n",
      "577/577 [==============================] - 649s 1s/step - loss: 0.0609\n",
      "Epoch 4/7\n",
      "577/577 [==============================] - 660s 1s/step - loss: 0.0357\n",
      "Epoch 5/7\n",
      "577/577 [==============================] - 680s 1s/step - loss: 0.0226\n",
      "Epoch 6/7\n",
      "577/577 [==============================] - 681s 1s/step - loss: 0.0156\n",
      "Epoch 7/7\n",
      "577/577 [==============================] - 730s 1s/step - loss: 0.0105\n",
      "The accuracy score is: 0.9309484418648802\n",
      "The f1 score is: 0.821232972897065\n",
      "Epoch 1/7\n",
      "577/577 [==============================] - 689s 1s/step - loss: 0.2756\n",
      "Epoch 2/7\n",
      "577/577 [==============================] - 650s 1s/step - loss: 0.1081\n",
      "Epoch 3/7\n",
      "577/577 [==============================] - 644s 1s/step - loss: 0.0593\n",
      "Epoch 4/7\n",
      "577/577 [==============================] - 632s 1s/step - loss: 0.0335\n",
      "Epoch 5/7\n",
      "577/577 [==============================] - 632s 1s/step - loss: 0.0208\n",
      "Epoch 6/7\n",
      "577/577 [==============================] - 634s 1s/step - loss: 0.0139\n",
      "Epoch 7/7\n",
      "577/577 [==============================] - 630s 1s/step - loss: 0.0090\n",
      "The accuracy score is: 0.9308399555205988\n",
      "The f1 score is: 0.8259148006553796\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Package for array math\n",
    "import numpy as np \n",
    "\n",
    "# Package for system path traversal\n",
    "import os\n",
    "\n",
    "# Package for working with dates\n",
    "from datetime import date\n",
    "\n",
    "# K fold analysis package\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Import the main analysis pipeline\n",
    "#from pipeline import Pipeline\n",
    "\n",
    "# Tensor creation class\n",
    "#from text_preprocessing import TextToTensor\n",
    "\n",
    "# Reading the configuration file\n",
    "import yaml\n",
    "with open(\"conf.yml\", 'r') as file:\n",
    "    conf = yaml.safe_load(file).get('pipeline')\n",
    "\n",
    "# Reading the stop words\n",
    "stop_words = []\n",
    "try:\n",
    "    stop_words = pd.read_csv('stop_words.txt', sep='\\n', header=None)[0].tolist()\n",
    "except Exception as e:\n",
    "    # This exception indicates that the file is missing or is in a bad format\n",
    "    print('Bad stop_words.txt file: {e}')\n",
    "\n",
    "# Reading the data\n",
    "train = pd.read_csv('clean_data.csv')\n",
    "test = pd.read_csv('Sexual Abusive Comments by Roma3 & INNO.csv')\n",
    "\n",
    "# Shuffling the data for the k fold analysis\n",
    "train = train.sample(frac=1)\n",
    "\n",
    "# Creating the input for the pipeline\n",
    "X_train = train['text'].tolist()\n",
    "Y_train = train['is_offensive'].tolist()\n",
    "\n",
    "X_test = test['Comment'].tolist()\n",
    "\n",
    "if conf.get('k_fold'):\n",
    "    kfold = KFold(n_splits=5)\n",
    "    acc = []\n",
    "    f1 = []\n",
    "    for train_index, test_index in kfold.split(X_train):\n",
    "        # Fitting the model and forecasting with a subset of data\n",
    "        k_results = Pipeline(\n",
    "            X_train=np.array(X_train)[train_index],\n",
    "            Y_train=np.array(Y_train)[train_index], \n",
    "            embed_path='glove.840B.300d.txt',\n",
    "            embed_dim=300,\n",
    "            X_test=np.array(X_train)[test_index],\n",
    "            Y_test=np.array(Y_train)[test_index],\n",
    "            max_len=conf.get('max_len'),\n",
    "            epochs=conf.get('epochs'),\n",
    "            batch_size=conf.get('batch_size')\n",
    "        )\n",
    "        # Saving the accuracy\n",
    "        acc += [k_results.acc]\n",
    "        f1 += [k_results.f1]\n",
    "        print(f'The accuracy score is: {acc[-1]}') \n",
    "        print(f'The f1 score is: {f1[-1]}') \n",
    "    print(f'Total mean accuracy is: {np.mean(acc)}')\n",
    "    print(f'Total mean f1 score is: {np.mean(f1)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "corporate-shame",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "721/721 [==============================] - 1203s 2s/step - loss: 0.2442\n",
      "Epoch 2/7\n",
      "721/721 [==============================] - 1074s 1s/step - loss: 0.0876\n",
      "Epoch 3/7\n",
      "721/721 [==============================] - 946s 1s/step - loss: 0.0478\n",
      "Epoch 4/7\n",
      "721/721 [==============================] - 983s 1s/step - loss: 0.0254\n",
      "Epoch 5/7\n",
      "721/721 [==============================] - 922s 1s/step - loss: 0.0140\n",
      "Epoch 6/7\n",
      "721/721 [==============================] - 905s 1s/step - loss: 0.0097\n",
      "Epoch 7/7\n",
      "721/721 [==============================] - 904s 1s/step - loss: 0.0072\n"
     ]
    }
   ],
   "source": [
    "# Running the pipeline with all the data\n",
    "results = Pipeline(\n",
    "    X_train=X_train,\n",
    "    Y_train=Y_train, \n",
    "    embed_path='glove.840B.300d.txt',\n",
    "    embed_dim=300,\n",
    "    stop_words=stop_words,\n",
    "    X_test=X_test,\n",
    "    max_len=conf.get('max_len'),\n",
    "    epochs=conf.get('epochs'),\n",
    "    batch_size=conf.get('batch_size')\n",
    ")\n",
    "\n",
    "# Some sanity checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bottom-training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0 515]] Score: 0.0010506808757781982\n",
      "Sentence: [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0   45 8274 2388   54]] Score: 0.9999997615814209\n"
     ]
    }
   ],
   "source": [
    "good = [\"fine\"]\n",
    "bad = [\"Fuck you bloody bitch\"]\n",
    "\n",
    "TextToTensor_instance = TextToTensor(\n",
    "tokenizer=results.tokenizer,\n",
    "max_len=conf.get('max_len')\n",
    ")\n",
    "\n",
    "# Converting to tensors\n",
    "good_nn = TextToTensor_instance.string_to_tensor(good)\n",
    "bad_nn = TextToTensor_instance.string_to_tensor(bad)\n",
    "\n",
    "# Forecasting\n",
    "p_good = results.model.predict(good_nn)[0][0]\n",
    "p_bad = results.model.predict(bad_nn)[0][0]\n",
    "\n",
    "print(f'Sentence: {good_nn} Score: {p_good}')\n",
    "print(f'Sentence: {bad_nn} Score: {p_bad}')\n",
    "\n",
    "# Saving the predictions\n",
    "test['prob_is_genuine'] = results.yhat\n",
    "test['target'] = [1 if x > 0.5 else 0 for x in results.yhat]\n",
    " \n",
    "# Saving the predictions to a csv file\n",
    "if conf.get('save_results'):\n",
    "    if not os.path.isdir('output'):\n",
    "        os.mkdir('output')    \n",
    "    test[['target']].to_csv(f'output/submission_{date.today()}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "violent-excuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import dill\n",
    "import joblib\n",
    "try:\n",
    "    joblib.dump(results,'results')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'model_v1.pk'\n",
    "with open('./'+filename, 'wb') as file:\n",
    "    joblib.dump(results, file) \n",
    "file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-sport",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
